# ğŸ‡¹ğŸ‡¼ TAIWAN-LLM æ•™å­¸æ–‡ä»¶ (ä¸­è‹±å°ç…§ç‰ˆ)

---

## ğŸ§  æ¦‚è¿° | Overview

**TAIWAN-LLM** æ˜¯ç”±åœ‹ç«‹è‡ºç£å¤§å­¸é–‹ç™¼çš„ç¬¬ä¸€å€‹é‡å° **å°ç£ç¹é«”ä¸­æ–‡èªè¨€èˆ‡æ–‡åŒ–** å¾®èª¿çš„å¤§å‹èªè¨€æ¨¡å‹ (Large Language Model, LLM)ã€‚  
å®ƒæ—¨åœ¨è§£æ±ºç›®å‰ä¸»æµ LLMï¼ˆå¦‚ GPT-4ã€Claude 2ï¼‰åå‘è‹±æ–‡èˆ‡ç°¡é«”ä¸­æ–‡èªæ–™ï¼Œå°è‡´æ¨¡å‹å°å°ç£èªå¢ƒç†è§£ä¸è¶³çš„å•é¡Œã€‚

> **TAIWAN-LLM** bridges linguistic and cultural gaps by fine-tuning on Traditional Chinese used in Taiwan, making it both linguistically precise and culturally aligned.

---

## ğŸ“š ç ”ç©¶å‹•æ©Ÿ | Motivation

| ä¸­æ–‡æ•˜è¿° | English Description |
|------------|------------------------|
| ç¾æœ‰ LLM å¤šä»¥è‹±æ–‡èˆ‡ç°¡é«”ä¸­æ–‡ç‚ºä¸»ï¼Œå°ç¹é«”ä¸­æ–‡ç†è§£ä¸è¶³ã€‚ | Most LLMs are trained on English and Simplified Chinese corpora, lacking proficiency in Traditional Chinese. |
| è‡ºç£ç¹é«”ä¸­æ–‡å…·ç¨ç‰¹èªå½™èˆ‡æ–‡åŒ–èƒŒæ™¯ã€‚ | Taiwanâ€™s Traditional Chinese carries distinct vocabulary and cultural nuances. |
| ç¼ºä¹åœ¨åœ°åŒ–èªè¨€æ¨¡å‹é€ æˆã€Œèªè¨€æ•¸ä½è½å·®ã€ã€‚ | This linguistic gap leads to a digital divide for Traditional Chinese users. |

---

## âš™ï¸ ä¸‰éšæ®µå¾®èª¿æ¶æ§‹ | Three-Stage Fine-Tuning Pipeline

```mermaid
flowchart LR
A[ğŸ“– Continue-Pretraining (cPT)] --> B[ğŸ—£ï¸ Supervised Fine-Tuning (SFT)]
B --> C[ğŸ’¬ Feedback Supervised Fine-Tuning (Feedback SFT)]
C --> D[ğŸš€ Evaluation & Deployment]
```

### 1ï¸âƒ£ Continue-Pretraining (cPT)
- åœ¨ LLaMA 2 åŸºç¤ä¸Šï¼ŒæŒçºŒæ–¼ **3,510 å„„å­—è© (35.1B tokens)** çš„å°ç£èªæ–™ä¸­å†è¨“ç·´ã€‚
- èªæ–™åˆ†å¸ƒï¼š
  | é¡åˆ¥ | æ–‡ä»¶æ•¸ | è©å…ƒæ•¸ (å„„) | ä½”æ¯” |
  |------|----------|-------------|------|
  | ç¤¾ç¾¤åª’é«” | 824 è¬ | 1660 | 47% |
  | æ–°è | 860 è¬ | 1040 | 30% |
  | çŸ¥è­˜åº« | 319 è¬ | 570 | 16% |
  | æ›¸ç± | 4000 | 240 | 7% |

### 2ï¸âƒ£ Supervised Fine-Tuning (SFT)
- ä½¿ç”¨å¤šè¼ªå°è©±è³‡æ–™é›†ï¼ˆAlpacaã€Flan V2ã€OpenAssistant ç­‰ï¼‰+
  **Taiwan Instruction Datasetï¼ˆäººå·¥æ’°å¯«åœ¨åœ°èªå¢ƒå°è©±ï¼‰**ã€‚

### 3ï¸âƒ£ Feedback Supervised Fine-Tuning (Feedback SFT)
- é€é [twllm.com](https://twllm.com) æ”¶é›† **20,000 ç­†ä½¿ç”¨è€…äº’å‹•å›é¥‹**ã€‚
- åƒ…ä½¿ç”¨ã€Œæ­£å‘è©•åƒ¹ã€æ¨£æœ¬å†æ¬¡å¾®èª¿ï¼Œä»¥è²¼è¿‘ä½¿ç”¨è€…æ–‡åŒ–åå¥½ã€‚

---

## ğŸ§ª å¯¦é©—è¨­è¨ˆ | Experimental Setup

| é …ç›® | èªªæ˜ |
|------|------|
| åŸºç¤æ¨¡å‹ | LLaMA 2 (7B / 13B) |
| è¨“ç·´ç’°å¢ƒ | 48 Ã— H100 GPU, bfloat16, DeepSpeed ZeRO-2, FlashAttention-2 |
| å„ªåŒ–å™¨ | AdamW (ç„¡ weight decay) |
| è©•æ¸¬å¹³å° | **TC-Eval Benchmark Suite** |
| ä»»å‹™é¡å‹ | å•ç­”ã€æ‘˜è¦ã€åˆ†é¡ã€è¡¨æ ¼ç†è§£ã€æ–‡åŒ–çŸ¥è­˜æ¸¬è©¦ |

---

## ğŸ“ˆ å¯¦é©—çµæœ | Results Summary

| æ¨¡å‹ | å¹³å‡åˆ†æ•¸ | å‚™è¨» |
|------|-----------|------|
| **TAIWAN-LLM (13B)** | **53.99%** | â‰ˆ GPT-3.5 Turbo |
| GPT-4 | 58.03% | å•†æ¥­å°é–‰æ¨¡å‹æœ€é«˜ |
| Claude 2.1 | 58.49% | æ–‡åŒ–ç†è§£å¼· |
| LLaMA 2 13B Chat | 34.06% | æ˜é¡¯è½å¾Œ |

**é—œéµè§€å¯Ÿ | Key Insights**  
- ç§»é™¤ cPT â†’ æ˜é¡¯é€€æ­¥ï¼ˆDRCD æº–ç¢ºç‡ç”± 87.6% â†’ 75.8%ï¼‰ã€‚  
- Feedback SFT æ•ˆæœè¼ƒå°ï¼Œä½†æå‡æ–‡åŒ–é©æ‡‰åº¦ã€‚  
- åŠ å…¥ä½å“è³ª Web è³‡æ–™ï¼ˆCommonCrawlï¼‰åè€Œé€ æˆæ€§èƒ½ä¸‹é™ã€‚

> "Data quality outweighs quantity â€” high-quality Taiwanese corpora are essential."

---

## ğŸ’¬ æ–‡åŒ–ç†è§£æ¸¬è©¦ | Cultural Understanding Examples

| å•é¡Œ | ChatGPT å›ç­” | TAIWAN-LLM å›ç­” |
|------|---------------|----------------|
| **NTU åœ¨å“ªè£¡ï¼Ÿ** | å—æ´‹ç†å·¥å¤§å­¸ï¼ˆæ–°åŠ å¡ï¼‰ | åœ‹ç«‹è‡ºç£å¤§å­¸ï¼ˆå°åŒ—å¸‚ï¼‰ |
| **ä»€éº¼æ˜¯ 22Kï¼Ÿ** | é‡‘çš„ç´”åº¦ç­‰ç´š | å¤§å­¸ç”Ÿèµ·è–ª 22,000 å…ƒä»£ç¨± |
| **è¼‰å…·æ˜¯ä»€éº¼ï¼Ÿ** | è³¼ç‰©è¢‹æˆ–å®¹å™¨ | é›»å­ç™¼ç¥¨è¼‰å…·ï¼ˆæ‰‹æ©Ÿ / APPï¼‰ |

âœ… çµè«–ï¼šTAIWAN-LLM åœ¨ã€Œæ–‡åŒ–èªå¢ƒå°é½Š (Cultural Alignment)ã€æ–¹é¢é¡¯è‘—å„ªæ–¼ä¸»æµæ¨¡å‹ã€‚

---

## ğŸ çµè«– | Conclusion

- **TAIWAN-LLM** æ˜¯å…¨çƒé¦–å€‹é‡å°å°ç£ç¹é«”ä¸­æ–‡æ–‡åŒ–å¾®èª¿çš„é–‹æºå¤§å‹èªè¨€æ¨¡å‹ã€‚
- ä¸‰éšæ®µè¨“ç·´æµç¨‹ï¼ˆcPT â†’ SFT â†’ Feedback SFTï¼‰æœ‰æ•ˆçµåˆèªè¨€èˆ‡æ–‡åŒ–å±¤é¢ã€‚
- æ¨¡å‹æ•ˆèƒ½å¯èˆ‡ GPT-3.5 Turbo åª²ç¾ï¼Œä¸”å…·å‚™æ›´å¼·çš„åœ¨åœ°æ–‡åŒ–ç†è§£åŠ›ã€‚
- å®Œå…¨é–‹æºï¼Œé¼“å‹µç¤¾ç¾¤å…±åŒé–‹ç™¼ï¼Œæ¨å‹•å°ç£ AI èªè¨€ä¸»æ¬Šã€‚

> **"Bridging the linguistic divide â€” empowering Traditional Chinese AI for Taiwan."**

---

## ğŸ”­ æœªä¾†æ–¹å‘ | Future Directions

- å°å…¥ **RLHF (Reinforcement Learning with Human Feedback)** èˆ‡ **DPO (Direct Preference Optimization)**ã€‚  
- æ“´å……å¤šèªæ··åˆï¼ˆå°èªã€å®¢èªã€åŸæ°‘èªï¼‰ã€‚  
- å¼·åŒ–çŸ¥è­˜æª¢ç´¢èˆ‡æŒçºŒå­¸ç¿’èƒ½åŠ›ã€‚

---

## ğŸ“ åƒè€ƒä¾†æº | References
- Lin, Yen-Ting & Chen, Yun-Nung (2023). *TAIWAN-LLM: Bridging the Linguistic Divide with a Culturally Aligned Language Model.* arXiv:2311.17487v1  
- [https://twllm.com](https://twllm.com)  
- [https://github.com/MiuLab/Taiwan-LLaMa](https://github.com/MiuLab/Taiwan-LLaMa)

---

ğŸ“˜ **ä½œè€…è¨»è¨˜**ï¼šæ­¤æ–‡ä»¶å¯ä½œç‚ºæ•™å­¸æ•™æã€LLM å¾®èª¿å…¥é–€æŒ‡å¼•ï¼Œæˆ–æ–‡åŒ–å°é½Šç ”ç©¶æ¡ˆä¾‹ã€‚

